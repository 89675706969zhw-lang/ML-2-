{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4098c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\anaconda3\\envs\\ind-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –®–∞–≥ 0: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 0: –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "# (Step 0: Import libraries and settings)\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (Display settings)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore') # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è\n",
    "\n",
    "print(\"‚úÖ –®–∞–≥ 0: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28675167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Loading Raw Data)...\n",
      "   -> –†–∞–∑–º–µ—Ä (Size): (145460, 23)\n",
      "   -> –ü—Ä–æ–ø—É—Å–∫–∏ –≤ —Ç–∞—Ä–≥–µ—Ç–µ (NaN in Target): 3267\n",
      "   -> –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ: (142193, 23)\n",
      "\n",
      "‚úÇÔ∏è 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ Train/Test (–¥–æ –æ—á–∏—Å—Ç–∫–∏)...\n",
      "\n",
      "üìä 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ EDA (report_train_raw.html)...\n",
      "   (–ó–¥–µ—Å—å –º—ã —É–≤–∏–¥–∏–º –ø—Ä–æ–ø—É—Å–∫–∏ –∏ –≤—ã–±—Ä–æ—Å—ã, –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç –∑–∞–¥–∞–Ω–∏–µ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:00<00:00, 4571.34it/s]00:00, 12.59it/s, Describe variable: RainTomorrow]\n",
      "Summarize dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:00<00:00, 66.86it/s, Completed]                      \n",
      "Generate report structure: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.23s/it]\n",
      "Render HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.19s/it]\n",
      "Export report to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 72.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω! –û—Ç–∫—Ä–æ–π—Ç–µ 'report_train_raw.html' —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏.\n",
      "\n",
      "üßπ 4. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Cleaning)...\n",
      "\n",
      "üíæ 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª—ã 'train_cleaned.csv' –∏ 'test_cleaned.csv' —Å–æ–∑–¥–∞–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 1: –ó–∞–≥—Ä—É–∑–∫–∞, EDA (–Ω–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö) –∏ –û—á–∏—Å—Ç–∫–∞\n",
    "# (Step 1: Load, Raw EDA, and Cleaning)\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filename = \"weatherAUS.csv\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print(\"üîÑ 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Loading Raw Data)...\")\n",
    "    raw_df = pd.read_csv(filename)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # A. –ü–µ—á–∞—Ç—å –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏(Print Raw Stats)\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"   -> –†–∞–∑–º–µ—Ä (Size): {raw_df.shape}\")\n",
    "    target = 'RainTomorrow'\n",
    "    print(f\"   -> –ü—Ä–æ–ø—É—Å–∫–∏ –≤ —Ç–∞—Ä–≥–µ—Ç–µ (NaN in Target): {raw_df[target].isna().sum()}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # B. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–∞ (Drop rows without Target)\n",
    "    # (–≠—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –î–û –æ—Ç—á–µ—Ç–∞, —Ç–∞–∫ –∫–∞–∫ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–∞ –∞–Ω–∞–ª–∏–∑ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω)\n",
    "    # ---------------------------------------------------------\n",
    "    df = raw_df.dropna(subset=[target]).copy()\n",
    "    print(f\"   -> –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ: {df.shape}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # C. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ Train/Test (–°–´–†–´–ï –î–ê–ù–ù–´–ï)\n",
    "    # (Split into Raw Train/Test)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n‚úÇÔ∏è 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ Train/Test (–¥–æ –æ—á–∏—Å—Ç–∫–∏)...\")\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º stratify, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤\n",
    "    train_raw, test_raw = train_test_split(df, test_size=0.2, random_state=42, stratify=df[target])\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # D. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ ydata-profiling (–ù–ê –°–´–†–´–• –î–ê–ù–ù–´–•)\n",
    "    # (Generate Report on RAW Data - To see Missing & Extreme values)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nüìä 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ EDA (report_train_raw.html)...\")\n",
    "    print(\"   (–ó–¥–µ—Å—å –º—ã —É–≤–∏–¥–∏–º –ø—Ä–æ–ø—É—Å–∫–∏ –∏ –≤—ã–±—Ä–æ—Å—ã, –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç –∑–∞–¥–∞–Ω–∏–µ)\")\n",
    "    \n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è Train, —á—Ç–æ–±—ã —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –≤—Ä–µ–º—è (–∏–ª–∏ –¥–ª—è –≤—Å–µ–≥–æ, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ)\n",
    "    profile = ProfileReport(train_raw, title=\"EDA - Raw Training Data\", minimal=True)\n",
    "    profile.to_file(\"report_train_raw.html\")\n",
    "    print(\"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω! –û—Ç–∫—Ä–æ–π—Ç–µ 'report_train_raw.html' —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # E. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Cleaning Data)\n",
    "    # (–¢–µ–ø–µ—Ä—å —á–∏—Å—Ç–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nüßπ 4. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Cleaning)...\")\n",
    "\n",
    "    def clean_data(data):\n",
    "        d = data.copy()\n",
    "        # 1. Date processing\n",
    "        if 'Date' in d.columns:\n",
    "            d['Date'] = pd.to_datetime(d['Date'])\n",
    "            d['Year'] = d['Date'].dt.year\n",
    "            d['Month'] = d['Date'].dt.month\n",
    "            d['Day'] = d['Date'].dt.day\n",
    "            d = d.drop(columns=['Date'])\n",
    "        \n",
    "        # 2. Fill Missing Values (Imputation)\n",
    "        # Numeric -> Median\n",
    "        num_cols = d.select_dtypes(include=[np.number]).columns\n",
    "        for col in num_cols:\n",
    "            d[col] = d[col].fillna(d[col].median())\n",
    "            \n",
    "        # Categorical -> Mode\n",
    "        cat_cols = d.select_dtypes(include=['object']).columns\n",
    "        for col in cat_cols:\n",
    "            if d[col].isna().sum() > 0:\n",
    "                d[col] = d[col].fillna(d[col].mode()[0])\n",
    "        \n",
    "        return d\n",
    "\n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –æ—á–∏—Å—Ç–∫–∏\n",
    "    train_cleaned = clean_data(train_raw)\n",
    "    test_cleaned = clean_data(test_raw)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # F. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (Saving)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nüíæ 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    train_cleaned.to_csv(\"train_cleaned.csv\", index=False)\n",
    "    test_cleaned.to_csv(\"test_cleaned.csv\", index=False)\n",
    "    print(\"‚úÖ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª—ã 'train_cleaned.csv' –∏ 'test_cleaned.csv' —Å–æ–∑–¥–∞–Ω—ã.\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0560702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ EDA (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)...\n",
      "   -> –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –¥–ª—è Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<?, ?it/s]30 [00:00<00:00, 11.60it/s, Describe variable: Day]     \n",
      "Summarize dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00<00:00, 64.71it/s, Completed]               \n",
      "Generate report structure: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.16s/it]\n",
      "Render HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.49it/s]\n",
      "Export report to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 164.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –¥–ª—è Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<?, ?it/s]30 [00:00<00:00, 76.20it/s, Describe variable: Day]        \n",
      "Summarize dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00<00:00, 111.61it/s, Completed]              \n",
      "Generate report structure: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.97s/it]\n",
      "Render HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.63it/s]\n",
      "Export report to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 80.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ! –û—Ç—á–µ—Ç—ã 'report_train.html' –∏ 'report_test.html' —Å–æ–∑–¥–∞–Ω—ã.\n",
      "   (–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã –≤ –º–µ–Ω—é —Å–ª–µ–≤–∞)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ EDA\n",
    "# (Step 2: Generate EDA Report using ydata-profiling)\n",
    "# ==========================================\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ —á—Ç–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
    "train_df = pd.read_csv(\"train_cleaned.csv\")\n",
    "test_df = pd.read_csv(\"test_cleaned.csv\")\n",
    "\n",
    "print(\"üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ EDA (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)...\")\n",
    "\n",
    "# 1. –û—Ç—á–µ—Ç –¥–ª—è Train (–æ—Å–Ω–æ–≤–Ω–æ–π)\n",
    "# (Generating report for Train data)\n",
    "print(\"   -> –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –¥–ª—è Train...\")\n",
    "profile_train = ProfileReport(\n",
    "    train_df, \n",
    "    title=\"EDA Report - Train Data\", \n",
    "    minimal=True # 'minimal=True' —É—Å–∫–æ—Ä—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å\n",
    ")\n",
    "profile_train.to_file(\"report_train.html\")\n",
    "\n",
    "# 2. –û—Ç—á–µ—Ç –¥–ª—è Test\n",
    "# (Generating report for Test data)\n",
    "print(\"   -> –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –¥–ª—è Test...\")\n",
    "profile_test = ProfileReport(\n",
    "    test_df, \n",
    "    title=\"EDA Report - Test Data\", \n",
    "    minimal=True\n",
    ")\n",
    "profile_test.to_file(\"report_test.html\")\n",
    "\n",
    "print(\"\\n‚úÖ –ì–æ—Ç–æ–≤–æ! –û—Ç—á–µ—Ç—ã 'report_train.html' –∏ 'report_test.html' —Å–æ–∑–¥–∞–Ω—ã.\")\n",
    "print(\"   (–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã –≤ –º–µ–Ω—é —Å–ª–µ–≤–∞)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfc604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ [–®–∞–≥ 3] –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\n",
      "   -> –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ (Target Encoding)...\n",
      "üìù –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n",
      "\n",
      "‚öôÔ∏è –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤ (Applying Encoders)...\n",
      "‚úÖ –®–∞–≥ 3 –≤—ã–ø–æ–ª–Ω–µ–Ω: –î–∞–Ω–Ω—ã–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "# (Step 3: Data Preparation & Feature Encoding)\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "print(\"üîÑ [–®–∞–≥ 3] –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "# –ß–∏—Ç–∞–µ–º —É–∂–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –®–∞–≥–∞ 1\n",
    "train_df = pd.read_csv(\"train_cleaned.csv\")\n",
    "test_df = pd.read_csv(\"test_cleaned.csv\")\n",
    "\n",
    "target_col = 'RainTomorrow'\n",
    "\n",
    "# 1. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ —á–∏—Å–ª–∞ (0/1)\n",
    "# (Convert Target 'Yes'/'No' to 0/1)\n",
    "if train_df[target_col].dtype == 'object':\n",
    "    print(\"   -> –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ (Target Encoding)...\")\n",
    "    le = LabelEncoder()\n",
    "    train_df[target_col] = le.fit_transform(train_df[target_col])\n",
    "    test_df[target_col] = le.transform(test_df[target_col])\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ (X) –∏ —Ü–µ–ª—å (y)\n",
    "X_train = train_df.drop(columns=[target_col])\n",
    "y_train = train_df[target_col]\n",
    "X_test = test_df.drop(columns=[target_col])\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"üìù –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {categorical_cols}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤ (Applying Encoders)...\")\n",
    "\n",
    "# 2. WoE Encoding (–¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)\n",
    "# (Weight of Evidence for Logistic Regression)\n",
    "woe_encoder = ce.WOEEncoder(cols=categorical_cols)\n",
    "X_train_woe = woe_encoder.fit_transform(X_train, y_train)\n",
    "X_test_woe = woe_encoder.transform(X_test)\n",
    "\n",
    "# 3. Target Encoding (–¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π)\n",
    "# (Target Encoding for Trees: RF, XGBoost)\n",
    "te_encoder = ce.TargetEncoder(cols=categorical_cols)\n",
    "X_train_te = te_encoder.fit_transform(X_train, y_train)\n",
    "X_test_te = te_encoder.transform(X_test)\n",
    "\n",
    "# 4. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ –¥–ª—è –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏)\n",
    "scaler = StandardScaler()\n",
    "X_train_lr = scaler.fit_transform(X_train_woe)\n",
    "X_test_lr = scaler.transform(X_test_woe)\n",
    "\n",
    "print(\"‚úÖ –®–∞–≥ 3 –≤—ã–ø–æ–ª–Ω–µ–Ω: –î–∞–Ω–Ω—ã–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff30ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nlp' extra dependency package 'fasttext-numpy2' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependency package 'nltk' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependency package 'transformers' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "üî• [–®–∞–≥ 4] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...\n",
      "\n",
      "1Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ Logistic Regression (–Ω–∞ WoE –¥–∞–Ω–Ω—ã—Ö)...\n",
      "\n",
      "2Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ Random Forest (–Ω–∞ TargetEnc –¥–∞–Ω–Ω—ã—Ö)...\n",
      "\n",
      "3Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ XGBoost (–Ω–∞ TargetEnc –¥–∞–Ω–Ω—ã—Ö)...\n",
      "\n",
      "4Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ LightAutoML (–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º DataFrame)...\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "# (Step 4: Training Models)\n",
    "# ==========================================\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç –º–æ–¥–µ–ª–µ–π\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
    "from lightautoml.tasks import Task\n",
    "\n",
    "print(\"üî• [–®–∞–≥ 4] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...\")\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (—á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –®–∞–≥–µ 5)\n",
    "predictions = {}\n",
    "\n",
    "# --- –ú–æ–¥–µ–ª—å 1: Logistic Regression ---\n",
    "print(\"\\n1Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ Logistic Regression (–Ω–∞ WoE –¥–∞–Ω–Ω—ã—Ö)...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_lr, y_train)\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞ 1\n",
    "predictions['Logistic Regression'] = lr_model.predict_proba(X_test_lr)[:, 1]\n",
    "\n",
    "# --- –ú–æ–¥–µ–ª—å 2: Random Forest ---\n",
    "print(\"\\n2Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ Random Forest (–Ω–∞ TargetEnc –¥–∞–Ω–Ω—ã—Ö)...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)\n",
    "rf_model.fit(X_train_te, y_train)\n",
    "predictions['Random Forest'] = rf_model.predict_proba(X_test_te)[:, 1]\n",
    "\n",
    "# --- –ú–æ–¥–µ–ª—å 3: XGBoost ---\n",
    "print(\"\\n3Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ XGBoost (–Ω–∞ TargetEnc –¥–∞–Ω–Ω—ã—Ö)...\")\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_te, y_train)\n",
    "predictions['XGBoost'] = xgb_model.predict_proba(X_test_te)[:, 1]\n",
    "\n",
    "# --- –ú–æ–¥–µ–ª—å 4: LightAutoML ---\n",
    "print(\"\\n4Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ LightAutoML (–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º DataFrame)...\")\n",
    "task = Task('binary')\n",
    "roles = {'target': target_col}\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–µ–∑ —è–≤–Ω–æ–≥–æ CV, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫\n",
    "automl = TabularAutoML(\n",
    "    task=task, \n",
    "    timeout=300,       # 5 –º–∏–Ω—É—Ç –ª–∏–º–∏—Ç\n",
    "    cpu_limit=-1, \n",
    "    reader_params={'n_jobs': 1, 'random_state': 42}\n",
    ")\n",
    "\n",
    "# –û–±—É—á–∞–µ–º –Ω–∞ train_df (LAMA —Å–∞–º–∞ –≤–Ω—É—Ç—Ä–∏ —Å–µ–±—è –¥–µ–ª–∏—Ç –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é)\n",
    "oof_pred = automl.fit_predict(train_df, roles=roles, verbose=0)\n",
    "test_pred = automl.predict(test_df)\n",
    "predictions['LightAutoML'] = test_pred.data[:, 0]\n",
    "\n",
    "print(\"\\n‚úÖ –®–∞–≥ 4 –≤—ã–ø–æ–ª–Ω–µ–Ω: –í—Å–µ 4 –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59417b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà [–®–∞–≥ 5] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...\n",
      "   -> Logistic Regression: AUC=0.8538\n",
      "   -> Random Forest: AUC=0.8594\n",
      "   -> XGBoost: AUC=0.8769\n",
      "   -> LightAutoML: AUC=0.8722\n",
      "\n",
      "‚úÖ –ì–†–ê–§–ò–ö –ì–û–¢–û–í! –°–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ 'roc_curve_final_submission.png'\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 5: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "# (Step 5: Visualization & Saving Results)\n",
    "# ==========================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤\n",
    "def plot_roc_with_ci(y_true, y_pred_proba, model_name, ax, n_bootstraps=50):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    \n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # Bootstrap\n",
    "    rng = np.random.RandomState(42)\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = resample(np.arange(len(y_true)), replace=True, random_state=rng)\n",
    "        if len(np.unique(y_true.iloc[indices])) < 2: continue\n",
    "        \n",
    "        y_true_boot = y_true.iloc[indices]\n",
    "        y_scores_boot = y_pred_proba[indices]\n",
    "        fpr_boot, tpr_boot, _ = roc_curve(y_true_boot, y_scores_boot)\n",
    "        aucs.append(auc(fpr_boot, tpr_boot))\n",
    "        interp_tpr = np.interp(mean_fpr, fpr_boot, tpr_boot)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "    \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = np.mean(aucs)\n",
    "    \n",
    "    auc_lower = np.percentile(aucs, 2.5)\n",
    "    auc_upper = np.percentile(aucs, 97.5)\n",
    "    tpr_lower = np.percentile(tprs, 2.5, axis=0)\n",
    "    tpr_upper = np.percentile(tprs, 97.5, axis=0)\n",
    "    \n",
    "    ax.plot(mean_fpr, mean_tpr, lw=2, alpha=0.8,\n",
    "            label=f'{model_name} (AUC = {mean_auc:.3f} [{auc_lower:.3f}-{auc_upper:.3f}])')\n",
    "    ax.fill_between(mean_fpr, tpr_lower, tpr_upper, color='gray', alpha=0.15)\n",
    "    print(f\"   -> {model_name}: AUC={mean_auc:.4f}\")\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "print(\"üìà [–®–∞–≥ 5] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...\")\n",
    "\n",
    "# –ü—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –≤—Å–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º\n",
    "for name, preds in predictions.items():\n",
    "    plot_roc_with_ci(y_test, preds, name, ax)\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance')\n",
    "ax.set_xlim([-0.05, 1.05])\n",
    "ax.set_ylim([-0.05, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves Comparison: 4 Models with 95% CI', fontsize=14)\n",
    "ax.legend(loc=\"lower right\", fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "output_filename = 'roc_curve_final_submission.png'\n",
    "fig.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\n‚úÖ –ì–†–ê–§–ò–ö –ì–û–¢–û–í! –°–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ '{output_filename}'\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ind-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
