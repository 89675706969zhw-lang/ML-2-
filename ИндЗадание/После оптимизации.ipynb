{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7aa189d0884c4c",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—ã–ø–∞–¥–µ–Ω–∏—è –æ—Å–∞–¥–∫–æ–≤ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å (RainTomorrow) –≤ –ê–≤—Å—Ç—Ä–∞–ª–∏–∏.\n",
    "(–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è vs —Ä–µ–≥—Ä–µ—Å—Å–∏—èÔºà–ø–æ–∞–∫ –Ω–µ—ÇÔºâ))\n",
    "(–ó–∞–¥–∞—ë—Ç –º–µ—Ç—Ä–∏–∫—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (auc –¥–ª—è binary, RMSE –¥–ª—è regression(–ø–æ–∫–∞ –Ω–µ—Ç)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4098c99b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T10:53:30.632768300Z",
     "start_time": "2026-02-07T10:53:30.571493900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –®–∞–≥ 0: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 0: –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "# (Step 0: Import libraries and settings)\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (Display settings)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore') # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è\n",
    "\n",
    "print(\"‚úÖ –®–∞–≥ 0: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28675167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T10:53:47.593644600Z",
     "start_time": "2026-02-07T10:53:30.641054200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Loading Raw Data)...\n",
      "\n",
      "üìä 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞ EDA (minimal=False)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:00<00:00, 53.84it/s]1<00:00, 20.80it/s, Describe variable: RainTomorrow]\n",
      "Summarize dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 289/289 [00:26<00:00, 10.85it/s, Completed]                           \n",
      "Generate report structure: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.90s/it]\n",
      "Render HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Export report to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 34.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω! (report_raw_full.html)\n",
      "\n",
      "üßê 3. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Data Diagnostics):\n",
      "   -> [–†–∞–∑–º–µ—Ä] –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: 145460, –°—Ç–æ–ª–±—Ü–æ–≤: 23\n",
      "   -> [–î—É–±–ª–∏–∫–∞—Ç—ã] –ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 0\n",
      "   -> [–¢–∞—Ä–≥–µ—Ç] –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ 'RainTomorrow': 3267\n",
      "   -> [–í—ã–±—Ä–æ—Å—ã] –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ú–µ—Ç–æ–¥ IQR):\n",
      "      - Rainfall: 25578 –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤\n",
      "      - WindGustSpeed: 3092 –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤\n",
      "      - WindSpeed3pm: 2523 –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤\n",
      "\n",
      "üõ†Ô∏è 4. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ—á–∏—Å—Ç–∫–∏ (Cleaning Actions):\n",
      "   -> üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ 3267 —Å—Ç—Ä–æ–∫ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–∞...\n",
      "   -> ‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–¥ —Å–ø–ª–∏—Ç–æ–º: (142193, 23)\n",
      "\n",
      "‚úÇÔ∏è 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ Train/Test (Stratified)...\n",
      "   -> Train: (113754, 23), Test: (28439, 23)\n",
      "\n",
      "üßπ 6. –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç...\n",
      "\n",
      "üíæ 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ! –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã, –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 1: –ó–∞–≥—Ä—É–∑–∫–∞, –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∏ –û—á–∏—Å—Ç–∫–∞\n",
    "# (Step 1: Load, Diagnostics, and Cleaning)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filename = \"weatherAUS.csv\"\n",
    "target = 'RainTomorrow'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print(\"üîÑ 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Loading Raw Data)...\")\n",
    "    raw_df = pd.read_csv(filename)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # A. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ (EDA Report)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nüìä 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞ EDA (minimal=False)...\")\n",
    "    # minimal=False: –í–∫–ª—é—á–∞–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã\n",
    "    profile = ProfileReport(raw_df, title=\"EDA - Full Raw Data\", minimal=False) \n",
    "    profile.to_file(\"report_raw_full.html\")\n",
    "    print(\"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω! (report_raw_full.html)\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # B. –ì–ª—É–±–æ–∫–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≤ –∫–æ–Ω—Å–æ–ª–∏ (Console Diagnostics)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nüßê 3. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Data Diagnostics):\")\n",
    "    \n",
    "    # 1. –†–∞–∑–º–µ—Ä (Size)\n",
    "    print(f\"   -> [–†–∞–∑–º–µ—Ä] –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {raw_df.shape[0]}, –°—Ç–æ–ª–±—Ü–æ–≤: {raw_df.shape[1]}\")\n",
    "    \n",
    "    # 2. –î—É–±–ª–∏–∫–∞—Ç—ã (Duplicates) - –í–ê–ñ–ù–û–ï –î–û–ë–ê–í–õ–ï–ù–ò–ï\n",
    "    duplicates_count = raw_df.duplicated().sum()\n",
    "    print(f\"   -> [–î—É–±–ª–∏–∫–∞—Ç—ã] –ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_count}\")\n",
    "    \n",
    "    # 3. –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (Target)\n",
    "    missing_target = raw_df[target].isna().sum()\n",
    "    print(f\"   -> [–¢–∞—Ä–≥–µ—Ç] –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ '{target}': {missing_target}\")\n",
    "    \n",
    "    # 4. –í—ã–±—Ä–æ—Å—ã (Outliers via IQR) - –í–ê–ñ–ù–û–ï –î–û–ë–ê–í–õ–ï–ù–ò–ï\n",
    "    # –ú—ã –Ω–µ —É–¥–∞–ª—è–µ–º –∏—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ (–ø–æ–≥–æ–¥–∞ –±—ã–≤–∞–µ—Ç —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–π), –Ω–æ –º—ã –¥–æ–ª–∂–Ω—ã –æ –Ω–∏—Ö –∑–Ω–∞—Ç—å.\n",
    "    print(\"   -> [–í—ã–±—Ä–æ—Å—ã] –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ú–µ—Ç–æ–¥ IQR):\")\n",
    "    numeric_cols = raw_df.select_dtypes(include=[np.number]).columns\n",
    "    outlier_info = []\n",
    "    for col in numeric_cols:\n",
    "        Q1 = raw_df[col].quantile(0.25)\n",
    "        Q3 = raw_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((raw_df[col] < (Q1 - 1.5 * IQR)) | (raw_df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "        if outliers > 0:\n",
    "            outlier_info.append((col, outliers))\n",
    "    \n",
    "    # –í—ã–≤–æ–¥ —Ç–æ–ø-3 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏\n",
    "    outlier_info.sort(key=lambda x: x[1], reverse=True)\n",
    "    for col, count in outlier_info[:3]:\n",
    "        print(f\"      - {col}: {count} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # C. \"–õ–µ—á–µ–Ω–∏–µ\" –¥–∞–Ω–Ω—ã—Ö (Processing Actions)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nüõ†Ô∏è 4. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ—á–∏—Å—Ç–∫–∏ (Cleaning Actions):\")\n",
    "    \n",
    "    # –î–µ–π—Å—Ç–≤–∏–µ 1: –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "    if duplicates_count > 0:\n",
    "        print(f\"   -> üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...\")\n",
    "        df = raw_df.drop_duplicates()\n",
    "    else:\n",
    "        df = raw_df.copy()\n",
    "\n",
    "    # –î–µ–π—Å—Ç–≤–∏–µ 2: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–∞\n",
    "    print(f\"   -> üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ {missing_target} —Å—Ç—Ä–æ–∫ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–∞...\")\n",
    "    df = df.dropna(subset=[target])\n",
    "    \n",
    "    print(f\"   -> ‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–¥ —Å–ø–ª–∏—Ç–æ–º: {df.shape}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # D. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ Train/Test\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n‚úÇÔ∏è 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ Train/Test (Stratified)...\")\n",
    "    train_raw, test_raw = train_test_split(df, test_size=0.2, random_state=42, stratify=df[target])\n",
    "    print(f\"   -> Train: {train_raw.shape}, Test: {test_raw.shape}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # E. –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (Imputation)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nüßπ 6. –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç...\")\n",
    "\n",
    "    def clean_data(data):\n",
    "        d = data.copy()\n",
    "        # –î–∞—Ç—ã\n",
    "        if 'Date' in d.columns:\n",
    "            d['Date'] = pd.to_datetime(d['Date'])\n",
    "            d['Year'] = d['Date'].dt.year\n",
    "            d['Month'] = d['Date'].dt.month\n",
    "            d['Day'] = d['Date'].dt.day\n",
    "            d = d.drop(columns=['Date'])\n",
    "        \n",
    "        # –ß–∏—Å–ª–∞ -> –ú–µ–¥–∏–∞–Ω–∞ (—É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º!)\n",
    "        num_cols = d.select_dtypes(include=[np.number]).columns\n",
    "        for col in num_cols:\n",
    "            d[col] = d[col].fillna(d[col].median())\n",
    "            \n",
    "        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ -> –ú–æ–¥–∞\n",
    "        cat_cols = d.select_dtypes(include=['object']).columns\n",
    "        for col in cat_cols:\n",
    "            if d[col].isna().sum() > 0:\n",
    "                d[col] = d[col].fillna(d[col].mode()[0])\n",
    "        return d\n",
    "\n",
    "    train_cleaned = clean_data(train_raw)\n",
    "    test_cleaned = clean_data(test_raw)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # F. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nüíæ 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
    "    train_cleaned.to_csv(\"train_cleaned.csv\", index=False)\n",
    "    test_cleaned.to_csv(\"test_cleaned.csv\", index=False)\n",
    "    print(\"‚úÖ –ì–æ—Ç–æ–≤–æ! –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã, –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0560702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T10:54:02.134833700Z",
     "start_time": "2026-02-07T10:53:48.156076600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\n",
      "\n",
      "üßê --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: [Train] ---\n",
      "   1. –†–∞–∑–º–µ—Ä (Shape): (113754, 25)\n",
      "   2. –ü—Ä–æ–ø—É—Å–∫–∏ (NaNs): ‚úÖ –ß–∏—Å—Ç–æ (Clean)\n",
      "   3. –î—É–±–ª–∏–∫–∞—Ç—ã (Duplicates): ‚úÖ –ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
      "   4. –í—ã–±—Ä–æ—Å—ã (Outliers): –í—Å–µ–≥–æ 115462 (—Å—É–º–º–∞ –ø–æ –≤—Å–µ–º –∫–æ–ª–æ–Ω–∫–∞–º)\n",
      "      (–ù–∞–ª–∏—á–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–≥–æ–¥—ã, –Ω–æ –º—ã —Å–ª–µ–¥–∏–º –∑–∞ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º)\n",
      "\n",
      "üßê --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: [Test] ---\n",
      "   1. –†–∞–∑–º–µ—Ä (Shape): (28439, 25)\n",
      "   2. –ü—Ä–æ–ø—É—Å–∫–∏ (NaNs): ‚úÖ –ß–∏—Å—Ç–æ (Clean)\n",
      "   3. –î—É–±–ª–∏–∫–∞—Ç—ã (Duplicates): ‚úÖ –ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
      "   4. –í—ã–±—Ä–æ—Å—ã (Outliers): –í—Å–µ–≥–æ 29125 (—Å—É–º–º–∞ –ø–æ –≤—Å–µ–º –∫–æ–ª–æ–Ω–∫–∞–º)\n",
      "      (–ù–∞–ª–∏—á–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–≥–æ–¥—ã, –Ω–æ –º—ã —Å–ª–µ–¥–∏–º –∑–∞ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º)\n",
      "\n",
      "‚öñÔ∏è –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –≤ Train ('RainTomorrow'):\n",
      "RainTomorrow\n",
      "No     0.775814\n",
      "Yes    0.224186\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ EDA (minimal=False)...\n",
      "   -> –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –¥–ª—è Train (report_train.html)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<?, ?it/s]30 [00:01<00:00,  6.52it/s, Describe variable: Day]\n",
      "Summarize dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [00:32<00:00, 12.08it/s, Completed]                           \n",
      "Generate report structure: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.28s/it]\n",
      "Render HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.62it/s]\n",
      "Export report to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –¥–ª—è Test (report_test.html)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<?, ?it/s]30 [00:00<00:00, 14.59it/s, Describe variable: Day] \n",
      "Summarize dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [00:26<00:00, 14.89it/s, Completed]                           \n",
      "Generate report structure: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.08s/it]\n",
      "Render HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.66it/s]\n",
      "Export report to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ! –û—Ç—á–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã –∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 2: –ì–ª—É–±–æ–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞\n",
    "# (Step 2: Deep Verification & Report Generation)\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "train_df = pd.read_csv(\"train_cleaned.csv\")\n",
    "test_df = pd.read_csv(\"test_cleaned.csv\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# A. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–ª—É–±–æ–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (Deep Check Function)\n",
    "# ---------------------------------------------------------\n",
    "def verify_dataset(df, name):\n",
    "    print(f\"\\nüßê --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: [{name}] ---\")\n",
    "    \n",
    "    # 1. –†–∞–∑–º–µ—Ä (Shape)\n",
    "    print(f\"   1. –†–∞–∑–º–µ—Ä (Shape): {df.shape}\")\n",
    "    \n",
    "    # 2. –ü—Ä–æ–ø—É—Å–∫–∏ (NaNs)\n",
    "    nans = df.isna().sum().sum()\n",
    "    status_nan = \"‚úÖ –ß–∏—Å—Ç–æ (Clean)\" if nans == 0 else f\"‚ö†Ô∏è –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ü–†–û–ü–£–°–ö–ò: {nans}\"\n",
    "    print(f\"   2. –ü—Ä–æ–ø—É—Å–∫–∏ (NaNs): {status_nan}\")\n",
    "    \n",
    "    # 3. –î—É–±–ª–∏–∫–∞—Ç—ã (Duplicates) - –ù–æ–≤–æ–µ!\n",
    "    dups = df.duplicated().sum()\n",
    "    status_dup = \"‚úÖ –ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\" if dups == 0 else f\"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç—ã: {dups}\"\n",
    "    print(f\"   3. –î—É–±–ª–∏–∫–∞—Ç—ã (Duplicates): {status_dup}\")\n",
    "    \n",
    "    # 4. –í—ã–±—Ä–æ—Å—ã (Outliers via IQR) - –ù–æ–≤–æ–µ!\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –≤—ã–±—Ä–æ—Å—ã –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    total_outliers = 0\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "        total_outliers += outliers\n",
    "    \n",
    "    print(f\"   4. –í—ã–±—Ä–æ—Å—ã (Outliers): –í—Å–µ–≥–æ {total_outliers} (—Å—É–º–º–∞ –ø–æ –≤—Å–µ–º –∫–æ–ª–æ–Ω–∫–∞–º)\")\n",
    "    print(\"      (–ù–∞–ª–∏—á–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–≥–æ–¥—ã, –Ω–æ –º—ã —Å–ª–µ–¥–∏–º –∑–∞ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º)\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# B. –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "# ---------------------------------------------------------\n",
    "verify_dataset(train_df, \"Train\")\n",
    "verify_dataset(test_df, \"Test\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# C. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤\n",
    "# ---------------------------------------------------------\n",
    "target = 'RainTomorrow'\n",
    "if target in train_df.columns:\n",
    "    print(f\"\\n‚öñÔ∏è –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –≤ Train ('{target}'):\")\n",
    "    print(train_df[target].value_counts(normalize=True))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# D. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ (Reports)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ EDA (minimal=False)...\")\n",
    "\n",
    "# –û—Ç—á–µ—Ç –¥–ª—è Train\n",
    "print(\"   -> –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –¥–ª—è Train (report_train.html)...\")\n",
    "profile_train = ProfileReport(\n",
    "    train_df, \n",
    "    title=\"EDA - Cleaned Train\", \n",
    "    minimal=False, \n",
    "    explorative=True\n",
    ")\n",
    "profile_train.to_file(\"report_train.html\")\n",
    "\n",
    "# –û—Ç—á–µ—Ç –¥–ª—è Test\n",
    "print(\"   -> –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –¥–ª—è Test (report_test.html)...\")\n",
    "profile_test = ProfileReport(\n",
    "    test_df, \n",
    "    title=\"EDA - Cleaned Test\", \n",
    "    minimal=False,\n",
    "    explorative=True\n",
    ")\n",
    "profile_test.to_file(\"report_test.html\")\n",
    "\n",
    "print(\"\\n‚úÖ –ì–æ—Ç–æ–≤–æ! –û—Ç—á–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã –∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bfc604f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T10:54:03.431002900Z",
     "start_time": "2026-02-07T10:54:02.177426300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ [–®–∞–≥ 3] –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\n",
      "   -> –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ (Target Encoding)...\n",
      "üìù –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n",
      "\n",
      "‚öôÔ∏è –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤ (Applying Encoders)...\n",
      "‚úÖ –®–∞–≥ 3 –≤—ã–ø–æ–ª–Ω–µ–Ω: –î–∞–Ω–Ω—ã–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "# (Step 3: Data Preparation & Feature Encoding)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "print(\"üîÑ [–®–∞–≥ 3] –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "# –ß–∏—Ç–∞–µ–º —É–∂–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –®–∞–≥–∞ 1\n",
    "train_df = pd.read_csv(\"train_cleaned.csv\")\n",
    "test_df = pd.read_csv(\"test_cleaned.csv\")\n",
    "\n",
    "target_col = 'RainTomorrow'\n",
    "\n",
    "# 1. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ —á–∏—Å–ª–∞ (0/1)\n",
    "# (Convert Target 'Yes'/'No' to 0/1)\n",
    "if train_df[target_col].dtype == 'object':\n",
    "    print(\"   -> –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ (Target Encoding)...\")\n",
    "    le = LabelEncoder()\n",
    "    train_df[target_col] = le.fit_transform(train_df[target_col])\n",
    "    test_df[target_col] = le.transform(test_df[target_col])\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ (X) –∏ —Ü–µ–ª—å (y)\n",
    "X_train = train_df.drop(columns=[target_col])\n",
    "y_train = train_df[target_col]\n",
    "X_test = test_df.drop(columns=[target_col])\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"üìù –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {categorical_cols}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤ (Applying Encoders)...\")\n",
    "\n",
    "# 2. WoE Encoding (–¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)\n",
    "# (Weight of Evidence for Logistic Regression)\n",
    "woe_encoder = ce.WOEEncoder(cols=categorical_cols)\n",
    "X_train_woe = woe_encoder.fit_transform(X_train, y_train)\n",
    "X_test_woe = woe_encoder.transform(X_test)\n",
    "\n",
    "# 3. Target Encoding (–¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π)\n",
    "# (Target Encoding for Trees: RF, XGBoost)\n",
    "te_encoder = ce.TargetEncoder(cols=categorical_cols)\n",
    "X_train_te = te_encoder.fit_transform(X_train, y_train)\n",
    "X_test_te = te_encoder.transform(X_test)\n",
    "\n",
    "# 4. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ –¥–ª—è –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏)\n",
    "scaler = StandardScaler()\n",
    "X_train_lr = scaler.fit_transform(X_train_woe)\n",
    "X_test_lr = scaler.transform(X_test_woe)\n",
    "\n",
    "print(\"‚úÖ –®–∞–≥ 3 –≤—ã–ø–æ–ª–Ω–µ–Ω: –î–∞–Ω–Ω—ã–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff30ff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T10:57:10.335672600Z",
     "start_time": "2026-02-07T10:54:03.574681500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nlp' extra dependency package 'fasttext-numpy2' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependency package 'nltk' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependency package 'transformers' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "üî• [–®–∞–≥ 4] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...\n",
      "\n",
      "1Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ Logistic Regression (–Ω–∞ WoE –¥–∞–Ω–Ω—ã—Ö)...\n",
      "\n",
      "2Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ Random Forest (–Ω–∞ TargetEnc –¥–∞–Ω–Ω—ã—Ö)...\n",
      "\n",
      "3Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ XGBoost (–Ω–∞ TargetEnc –¥–∞–Ω–Ω—ã—Ö)...\n",
      "\n",
      "4Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ LightAutoML (–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º DataFrame)...\n",
      "\n",
      "‚úÖ –®–∞–≥ 4 –≤—ã–ø–æ–ª–Ω–µ–Ω: –í—Å–µ 4 –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "# (Step 4: Training Models)\n",
    "# ==========================================\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç –º–æ–¥–µ–ª–µ–π\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
    "from lightautoml.tasks import Task\n",
    "\n",
    "print(\"üî• [–®–∞–≥ 4] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...\")\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (—á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –®–∞–≥–µ 5)\n",
    "predictions = {}\n",
    "\n",
    "# --- –ú–æ–¥–µ–ª—å 1: Logistic Regression ---\n",
    "print(\"\\n1Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ Logistic Regression (–Ω–∞ WoE –¥–∞–Ω–Ω—ã—Ö)...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_lr, y_train)\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞ 1\n",
    "predictions['Logistic Regression'] = lr_model.predict_proba(X_test_lr)[:, 1]\n",
    "\n",
    "# --- –ú–æ–¥–µ–ª—å 2: Random Forest ---\n",
    "print(\"\\n2Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ Random Forest (–Ω–∞ TargetEnc –¥–∞–Ω–Ω—ã—Ö)...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)\n",
    "rf_model.fit(X_train_te, y_train)\n",
    "predictions['Random Forest'] = rf_model.predict_proba(X_test_te)[:, 1]\n",
    "\n",
    "# --- –ú–æ–¥–µ–ª—å 3: XGBoost ---\n",
    "print(\"\\n3Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ XGBoost (–Ω–∞ TargetEnc –¥–∞–Ω–Ω—ã—Ö)...\")\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_te, y_train)\n",
    "predictions['XGBoost'] = xgb_model.predict_proba(X_test_te)[:, 1]\n",
    "\n",
    "# --- –ú–æ–¥–µ–ª—å 4: LightAutoML ---\n",
    "print(\"\\n4Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ LightAutoML (–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º DataFrame)...\")\n",
    "task = Task('binary')\n",
    "roles = {'target': target_col}\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–µ–∑ —è–≤–Ω–æ–≥–æ CV, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫\n",
    "automl = TabularAutoML(\n",
    "    task=task, \n",
    "    timeout=300,       # 5 –º–∏–Ω—É—Ç –ª–∏–º–∏—Ç\n",
    "    cpu_limit=-1, \n",
    "    reader_params={'n_jobs': 1, 'random_state': 42}\n",
    ")\n",
    "\n",
    "# –û–±—É—á–∞–µ–º –Ω–∞ train_df (LAMA —Å–∞–º–∞ –≤–Ω—É—Ç—Ä–∏ —Å–µ–±—è –¥–µ–ª–∏—Ç –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é)\n",
    "oof_pred = automl.fit_predict(train_df, roles=roles, verbose=0)\n",
    "test_pred = automl.predict(test_df)\n",
    "predictions['LightAutoML'] = test_pred.data[:, 0]\n",
    "\n",
    "print(\"\\n‚úÖ –®–∞–≥ 4 –≤—ã–ø–æ–ª–Ω–µ–Ω: –í—Å–µ 4 –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59417b3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T10:57:12.016197100Z",
     "start_time": "2026-02-07T10:57:10.419321900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà [–®–∞–≥ 5] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...\n",
      "   -> Logistic Regression: AUC=0.8668\n",
      "   -> Random Forest: AUC=0.8739\n",
      "   -> XGBoost: AUC=0.8976\n",
      "   -> LightAutoML: AUC=0.8961\n",
      "\n",
      "‚úÖ –ì–†–ê–§–ò–ö –ì–û–¢–û–í! –°–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ 'roc_curve_final_submission.png'\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# –®–ê–ì 5: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "# (Step 5: Visualization & Saving Results)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤\n",
    "# –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 1: –ê—Ä–≥—É–º–µ–Ω—Ç 'ax' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'plot_ax', —á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å –≤–Ω–µ—à–Ω–µ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
    "def plot_roc_with_ci(y_true, y_pred_proba, model_name, plot_ax, n_bootstraps=50):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    # Bootstrap\n",
    "    rng = np.random.RandomState(42)\n",
    "\n",
    "    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 2: –ó–∞–º–µ–Ω–∏–ª–∏ 'i' –Ω–∞ '_', —Ç–∞–∫ –∫–∞–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Ü–∏–∫–ª–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = resample(np.arange(len(y_true)), replace=True, random_state=rng)\n",
    "        if len(np.unique(y_true.iloc[indices])) < 2: continue\n",
    "\n",
    "        y_true_boot = y_true.iloc[indices]\n",
    "        y_scores_boot = y_pred_proba[indices]\n",
    "        fpr_boot, tpr_boot, _ = roc_curve(y_true_boot, y_scores_boot)\n",
    "        aucs.append(auc(fpr_boot, tpr_boot))\n",
    "        interp_tpr = np.interp(mean_fpr, fpr_boot, tpr_boot)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = np.mean(aucs)\n",
    "\n",
    "    auc_lower = np.percentile(aucs, 2.5)\n",
    "    auc_upper = np.percentile(aucs, 97.5)\n",
    "    tpr_lower = np.percentile(tprs, 2.5, axis=0)\n",
    "    tpr_upper = np.percentile(tprs, 97.5, axis=0)\n",
    "\n",
    "    # –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–π 'plot_ax' –≤–º–µ—Å—Ç–æ 'ax'\n",
    "    plot_ax.plot(mean_fpr, mean_tpr, lw=2, alpha=0.8,\n",
    "            label=f'{model_name} (AUC = {mean_auc:.3f} [{auc_lower:.3f}-{auc_upper:.3f}])')\n",
    "    plot_ax.fill_between(mean_fpr, tpr_lower, tpr_upper, color='gray', alpha=0.15)\n",
    "    print(f\"   -> {model_name}: AUC={mean_auc:.4f}\")\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "print(\"üìà [–®–∞–≥ 5] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...\")\n",
    "\n",
    "# –ü—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –≤—Å–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º\n",
    "for name, preds in predictions.items():\n",
    "    # –ü–µ—Ä–µ–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π 'ax' –≤ —Ñ—É–Ω–∫—Ü–∏—é, –≥–¥–µ –æ–Ω –±—É–¥–µ—Ç –ø—Ä–∏–Ω—è—Ç –∫–∞–∫ 'plot_ax'\n",
    "    plot_roc_with_ci(y_test, preds, name, ax)\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance')\n",
    "ax.set_xlim([-0.05, 1.05])\n",
    "ax.set_ylim([-0.05, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves Comparison: 4 Models with 95% CI', fontsize=14)\n",
    "ax.legend(loc=\"lower right\", fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "output_filename = 'roc_curve_final_submission.png'\n",
    "fig.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\n‚úÖ –ì–†–ê–§–ò–ö –ì–û–¢–û–í! –°–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ '{output_filename}'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e957cdfd3d637102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T11:01:24.169675600Z",
     "start_time": "2026-02-07T11:01:24.087729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ [–®–∞–≥ 6] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞...\n",
      "   -> ‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: best_model.pkl\n",
      "   -> ‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞.\n",
      "\n",
      "üîç –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (Inference Result):\n",
      "   -> –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (Input features): \n",
      "[[1.85252452e-01 1.51000000e+01 3.29000000e+01 0.00000000e+00\n",
      "  4.60000000e+00 8.50000000e+00 1.52037189e-01 5.00000000e+01\n",
      "  1.46405773e-01 2.54350567e-01 7.00000000e+00 1.10000000e+01\n",
      "  7.30000000e+01 3.20000000e+01 1.01550000e+03 1.01180000e+03\n",
      "  7.00000000e+00 5.00000000e+00 1.90000000e+01 3.03000000e+01\n",
      "  1.56275742e-01 2.01700000e+03 1.00000000e+00 2.60000000e+01]]\n",
      "   -> –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å (Predicted Class): 0\n",
      "   -> –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ–∂–¥—è (Probability): 0.1153\n",
      "   -> –†–µ–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (Actual Value): 0\n",
      "\n",
      "üéâ –ú–æ–¥–µ–ª—å —É–≥–∞–¥–∞–ª–∞ –≤–µ—Ä–Ω–æ! (Correct prediction)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  –®–ê–ì 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
    "# (Step 6: Save Best Model & Inference via .pkl)\n",
    "# =========================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(\"\\nüíæ [–®–∞–≥ 6] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞...\")\n",
    "\n",
    "# 1. –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "# (–í –≤–∞—à–µ–º –∫–æ–¥–µ XGBoost –∏–ª–∏ Random Forest —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –¥–∞–¥—É—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n",
    "# –ó–¥–µ—Å—å –º—ã –±–µ—Ä–µ–º XGBoost –∫–∞–∫ –ø—Ä–∏–º–µ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏)\n",
    "best_model = xgb_model\n",
    "best_model_name = \"best_model.pkl\"\n",
    "\n",
    "# 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ .pkl —Ñ–∞–π–ª \n",
    "# (Saving the model to a .pkl file)\n",
    "with open(best_model_name, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"   -> ‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: {best_model_name}\")\n",
    "\n",
    "# 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –æ–±—Ä–∞—Ç–Ω–æ (–ò–º–∏—Ç–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ)\n",
    "# (Loading the model back to simulate production environment)\n",
    "with open(best_model_name, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "print(\"   -> ‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞.\")\n",
    "\n",
    "# 4. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å 1 –∑–∞–ø–∏—Å–∏ (Inference of 1 record)\n",
    "# (–ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ X_test_te, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ —É–∂–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∞)\n",
    "sample_record = X_test_te.iloc[0:1]\n",
    "real_value = y_test.iloc[0]\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (Prediction)\n",
    "pred_class = loaded_model.predict(sample_record)[0]\n",
    "pred_proba = loaded_model.predict_proba(sample_record)[:, 1][0]\n",
    "\n",
    "print(\"\\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (Inference Result):\")\n",
    "print(f\"   -> –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (Input features): \\n{sample_record.values}\")\n",
    "print(f\"   -> –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å (Predicted Class): {pred_class}\")\n",
    "print(f\"   -> –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ–∂–¥—è (Probability): {pred_proba:.4f}\")\n",
    "print(f\"   -> –†–µ–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (Actual Value): {real_value}\")\n",
    "\n",
    "if pred_class == real_value:\n",
    "    print(\"\\nüéâ –ú–æ–¥–µ–ª—å —É–≥–∞–¥–∞–ª–∞ –≤–µ—Ä–Ω–æ! (Correct prediction)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è –ú–æ–¥–µ–ª—å –æ—à–∏–±–ª–∞—Å—å. (Incorrect prediction)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ind-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
